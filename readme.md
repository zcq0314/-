# ACL
## nlp的可解释性与分析
### 2023
* Entity Tracking in Language Models

## 对抗攻击和鲁棒性
### 2023
* How do humans perceive adversarial text? A reality check on the validity and naturalness of word-based adversarial attacks[[PDF](https://arxiv.org/pdf/2305.15587.pdf)]

* Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications[[PDF](https://arxiv.org/pdf/2305.06522.pdf)]
* Text Adversarial Purification as Defense against Adversarial Attacks[[PDF](https://arxiv.org/pdf/2203.14207.pdf)]
* White-Box Multi-Objective Adversarial Attack on Dialogue Generation[[PDF](https://arxiv.org/pdf/2305.03655.pdf)]
* Contrastive Learning with Adversarial Examples for Alleviating Pathology of Language Model[[PDF](https://aclanthology.org/2023.acl-long.358.pdf)]

### 2022
* Adversarial Authorship Attribution for Deobfuscation[[PDF](https://aclanthology.org/2022.acl-long.509.pdf)][[Code](https://github.com/reginazhai/Authorship-Deobfuscation)]

* Adversarial Soft Prompt Tuning for Cross-Domain Sentiment Analysis[[PDF](https://aclanthology.org/2022.acl-long.174.pdf)]

* Flooding-X: Improving BERT’s Resistance to Adversarial Attacks via LossRestricted Fine-Tuning[[PDF](https://aclanthology.org/2022.acl-long.386.pdf)]

* Imputing Out-of-Vocabulary Embeddings with LOVE Makes Language Models Robust with Little Cost[[PDF](https://arxiv.org/pdf/2203.07860.pdf)][[Code](https://github.com/tigerchen52/LOVE)]

* ParaDetox: Detoxification with Parallel Data[[PDF](https://aclanthology.org/2022.acl-long.469.pdf)][[Code](https://github.com/s-nlp/paradetox)]

* Pass off Fish Eyes for Pearls: Attacking Model Selection of Pre-trained Models[[PDF](https://aclanthology.org/2022.acl-long.347.pdf)][[Code](https://github.com/thunlp/Model-Selection-Attack)]

* SHIELD: Defending Textual Neural Networks against Multiple Black-Box[[PDF](https://aclanthology.org/2022.acl-long.459.pdf)][[Code](https://github.com/lethaiq/shield-defend-adversarial-texts)]

* Towards Robustness of Text-to-SQL Models Against Natural and Realistic Adversarial Table Perturbation[[PDF](https://aclanthology.org/2022.acl-long.142.pdf)][[Code](https://github.com/microsoft/ContextualSP)]
#### EMNLP
* Character-level White-Box Adversarial Attacks against Transformers via Attachable Subwords Substitution[[PDF](https://arxiv.org/pdf/2210.17004.pdf)][[Code](https://github.com/THU-BPM/CWBA)]
* （双答案句子攻击问答模型） TASA: Deceiving Question Answering Models by Twin Answer Sentences Attack[[PDF](https://arxiv.org/pdf/2210.15221.pdf)][[Code](https://github.com/caoyu-noob/TASA)]
* Textual Manifold-based Defense Against Natural Language Adversarial Examples[[PDF](https://arxiv.org/pdf/2211.02878.pdf)][[Code](https://github.com/dangne/tmd)]
* Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial NLP[[PDF](https://arxiv.org/pdf/2210.10683.pdf)]
### 2021
* Improving Gradient-based Adversarial Training for Text Classification by Contrastive Learning and Auto-Encoder[[PDF](https://arxiv.org/pdf/2109.06536v1.pdf)]
* Defense against Synonym Substitution-based Adversarial Attacks via Dirichlet Neighborhood Ensemble

* A Sweet Rabbit Hole by DARCY: Using Honeypots to Detect Universal Trigger’s Adversarial Attacks

* Crafting Adversarial Examples for Neural Machine Translation

* Adversarial Learning for Discourse Rhetorical Structure Parsing

* Reliability Testing for Natural Language Processing Systems

* Robust Knowledge Graph Completion with Stacked Convolutions and a Student Re-Ranking Network

* Towards Robustness of Text-to-SQL Models against Synonym Substitution

* Improving Paraphrase Detection with the Adversarial Paraphrasing Task

* MATE-KD: Masked Adversarial TExt, a Companion to Knowledge Distillation

* On the Efficacy of Adversarial Data Collection for Question Answering: Results from a Large-Scale Randomized Study

* WARP: Word-level Adversarial ReProgramming
* Improving Arabic Diacritization with Regularized Decoding and Adversarial Training

* An Empirical Study on Adversarial Attack on NMT: Languages and Positions Matter

* Using Adversarial Attacks to Reveal the Statistical Bias in Machine Reading Comprehension Models

* OutFlip: Generating Examples for Unknown Intent Detection with Natural Language Attack

## 鲁棒性提高公平性
### 2021
* Does Robustness Improve Fairness? Approaching Fairness with Word Substitution Robustness Methods for Text Classification[[PDF](https://arxiv.org/pdf/2106.10826.pdf)]

## 中文
### 2021
* Correcting Chinese Spelling Errors with Phonetic Pre-training
* Dynamic Connected Networks for Chinese Spelling Check
## 关于语言模型的攻防
### 2023
* Language model acceptability judgements are not always robust to context[[PDF]()]
### 2021
* BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively Inspired Orthographic Adversarial Attacks[[PDF](https://arxiv.org/pdf/2106.01452.pdf)]
* Defending Pre-trained Language Models from Adversarial Word Substitutions Without Performance Sacrifice[[PDF](https://arxiv.org/pdf/2105.14553.pdf)]
## 优化器
### 2023
* CAME: Confidence-guided Adaptive Memory Efficient Optimization[[PDF](https://arxiv.org/pdf/2307.02047.pdf)][Code](https://github.com/yangluo7/CAME)]
## 分类器
### 2023
* Linear Classifier: An Often-Forgotten Baseline for Text Classification
