
# ACL

## 对抗攻击和鲁棒性
#### 2023
* How do humans perceive adversarial text? A reality check on the validity and naturalness of word-based adversarial attacks[[PDF](https://arxiv.org/pdf/2305.15587.pdf)]

* Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications[[PDF](https://arxiv.org/pdf/2305.06522.pdf)]
* Text Adversarial Purification as Defense against Adversarial Attacks[[PDF](https://arxiv.org/pdf/2203.14207.pdf)]
* White-Box Multi-Objective Adversarial Attack on Dialogue Generation[[PDF](https://arxiv.org/pdf/2305.03655.pdf)]
* Contrastive Learning with Adversarial Examples for Alleviating Pathology of Language Model[[PDF](https://aclanthology.org/2023.acl-long.358.pdf)]

#### 2022
* Adversarial Authorship Attribution for Deobfuscation[[PDF](https://aclanthology.org/2022.acl-long.509.pdf)][[Code](https://github.com/reginazhai/Authorship-Deobfuscation)]

* Adversarial Soft Prompt Tuning for Cross-Domain Sentiment Analysis[[PDF](https://aclanthology.org/2022.acl-long.174.pdf)]

* Flooding-X: Improving BERT’s Resistance to Adversarial Attacks via LossRestricted Fine-Tuning[[PDF](https://aclanthology.org/2022.acl-long.386.pdf)]

* Imputing Out-of-Vocabulary Embeddings with LOVE Makes Language Models Robust with Little Cost[[PDF](https://arxiv.org/pdf/2203.07860.pdf)][[Code](https://github.com/tigerchen52/LOVE)]

* ParaDetox: Detoxification with Parallel Data[[PDF](https://aclanthology.org/2022.acl-long.469.pdf)][[Code](https://github.com/s-nlp/paradetox)]

* Pass off Fish Eyes for Pearls: Attacking Model Selection of Pre-trained Models[[PDF](https://aclanthology.org/2022.acl-long.347.pdf)][[Code](https://github.com/thunlp/Model-Selection-Attack)]

* SHIELD: Defending Textual Neural Networks against Multiple Black-Box[[PDF](https://aclanthology.org/2022.acl-long.459.pdf)][[Code](https://github.com/lethaiq/shield-defend-adversarial-texts)]

* Towards Robustness of Text-to-SQL Models Against Natural and Realistic Adversarial Table Perturbation[[PDF](https://aclanthology.org/2022.acl-long.142.pdf)][[Code](https://github.com/microsoft/ContextualSP)]
【EMNLP】
* Character-level White-Box Adversarial Attacks against Transformers via Attachable Subwords Substitution[[PDF](https://arxiv.org/pdf/2210.17004.pdf)][[Code](https://github.com/THU-BPM/CWBA)]
* （双答案句子攻击问答模型） TASA: Deceiving Question Answering Models by Twin Answer Sentences Attack[[PDF](https://arxiv.org/pdf/2210.15221.pdf)][[Code](https://github.com/caoyu-noob/TASA)]
* Textual Manifold-based Defense Against Natural Language Adversarial Examples[[PDF](https://arxiv.org/pdf/2211.02878.pdf)][[Code](https://github.com/dangne/tmd)]
* Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial NLP[[PDF](https://arxiv.org/pdf/2210.10683.pdf)]

【COLING】
* Semantic-Preserving Adversarial Code Comprehension[[PDF](https://aclanthology.org/2022.coling-1.267.pdf)]
* PARSE: An Efficient Search Method for Black-box Adversarial Text Attacks[[PDF](https://aclanthology.org/2022.coling-1.423.pdf)]
* PAEG: Phrase-level Adversarial Example Generation for Neural Machine Translation[[PDF](https://aclanthology.org/2022.coling-1.451.pdf)]
* (最小删除引起的神经机器翻译错误）Rare but Severe Neural Machine Translation Errors Induced by Minimal Deletion: An Empirical Study on Chinese and English[[PDF](https://aclanthology.org/2022.coling-1.459.pdf)]

【NAACL】
* ValCAT: Variable-Length Contextualized Adversarial Transformations Using Encoder-Decoder Language Model[[PDF](https://aclanthology.org/2022.naacl-main.125.pdf)]

#### 2021
* Improving Gradient-based Adversarial Training for Text Classification by Contrastive Learning and Auto-Encoder[[PDF](https://arxiv.org/pdf/2109.06536v1.pdf)]
* Defense against Synonym Substitution-based Adversarial Attacks via Dirichlet Neighborhood Ensemble

* A Sweet Rabbit Hole by DARCY: Using Honeypots to Detect Universal Trigger’s Adversarial Attacks

* Crafting Adversarial Examples for Neural Machine Translation

* Adversarial Learning for Discourse Rhetorical Structure Parsing

* Reliability Testing for Natural Language Processing Systems

* Robust Knowledge Graph Completion with Stacked Convolutions and a Student Re-Ranking Network

* Towards Robustness of Text-to-SQL Models against Synonym Substitution

* Improving Paraphrase Detection with the Adversarial Paraphrasing Task

* MATE-KD: Masked Adversarial TExt, a Companion to Knowledge Distillation

* On the Efficacy of Adversarial Data Collection for Question Answering: Results from a Large-Scale Randomized Study

* WARP: Word-level Adversarial ReProgramming
* Improving Arabic Diacritization with Regularized Decoding and Adversarial Training

* An Empirical Study on Adversarial Attack on NMT: Languages and Positions Matter

* Using Adversarial Attacks to Reveal the Statistical Bias in Machine Reading Comprehension Models

* OutFlip: Generating Examples for Unknown Intent Detection with Natural Language Attack
【EMNLP】
* Achieving Model Robustness through Discrete Adversarial Training[[PDF](https://arxiv.org/pdf/2106.10826.pdf)]
* Multi-granularity Textual Adversarial Attack with Behavior Cloning[[PDF](https://arxiv.org/pdf/2109.04367.pdf)]
* （评估神经语言模型对输入干扰的鲁棒性）Evaluating the Robustness of Neural Language Models to Input Perturbations[[PDF](https://arxiv.org/pdf/2109.04367.pdf)]
* （针对跨语言知识图谱对齐的对抗性攻击）Adversarial Attack against Cross-lingual Knowledge Graph Alignment
* （针对输入鲁棒性的基于特征的对抗元嵌入）FAME: Feature-Based Adversarial Meta-Embeddings for Robust Input Representations
* （通过实例归因方法对知识图谱嵌入的对抗性攻击）Adversarial Attacks on Knowledge Graph Embeddings via Instance Gradient-based Adversarial Attacks against Text TransformersAttribution Methods
* （黑盒环境中查询高效攻击的强大基线）A Strong Baseline for Query Efficient Attacks in a Black Box Setting
* Gradient-based Adversarial Attacks against Text Transformers[[PDF](https://arxiv.org/pdf/2104.13733.pdf)]
* Searching for an Effective Defender: Benchmarking Defense against Adversarial Word Substitution[[PDF](https://arxiv.org/pdf/2108.12777.pdf)]
* On the Transferability of Adversarial Attacks against Neural Text Classifier[[PDF](https://arxiv.org/pdf/2011.08558.pdf)]
* Contrasting Human- and Machine-Generated Word-Level Adversarial Examples for Text Classification[[PDF](https://arxiv.org/pdf/2109.04385.pdf)]
### 1.1 神经机器翻译
#### 2022
【NAACL】
* Generating Authentic Adversarial Examples beyond Meaning-preserving with Doubly Round-trip Translation[[PDF](https://aclanthology.org/2022.naacl-main.316.pdf)]

## 2、对抗攻击的应用
#### 2022
【NAACL】
* (一句话值一千美元：对推文的对抗性攻击傻瓜股票预测)A Word is Worth A Thousand Dollars: Adversarial Attack on Tweets Fools Stock Prediction[[PDF](https://arxiv.org/pdf/2109.04385.pdf)]

## 3、对抗攻击的检测和防御
#### 2022
【NAACL】
* （不为小事操心，只为其他事分类： 样本屏蔽保护文本分类器免受对抗性攻击）Don’t sweat the small stuff, classify the rest: Sample Shielding to protect text classifiers against adversarial attacks[[PDF](https://aclanthology.org/2022.naacl-main.195.pdf)]
* Residue-Based Natural Language Adversarial Attack Detection[[PDF](https://aclanthology.org/2022.naacl-main.281.pdf)]
## 4、nlp的可解释性与分析
#### 2023
* Entity Tracking in Language Models
## 5、鲁棒性提高公平性
#### 2021
* Does Robustness Improve Fairness? Approaching Fairness with Word Substitution Robustness Methods for Text Classification[[PDF](https://arxiv.org/pdf/2106.10826.pdf)]

## 中文
#### 2021
* Correcting Chinese Spelling Errors with Phonetic Pre-training
* Dynamic Connected Networks for Chinese Spelling Check
## 关于语言模型的攻防
#### 2023
* Language model acceptability judgements are not always robust to context[[PDF]()]
#### 2021
* BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively Inspired Orthographic Adversarial Attacks[[PDF](https://arxiv.org/pdf/2106.01452.pdf)]
* Defending Pre-trained Language Models from Adversarial Word Substitutions Without Performance Sacrifice[[PDF](https://arxiv.org/pdf/2105.14553.pdf)]
## 优化器
#### 2023
* CAME: Confidence-guided Adaptive Memory Efficient Optimization[[PDF](https://arxiv.org/pdf/2307.02047.pdf)][Code](https://github.com/yangluo7/CAME)]
## 分类器
#### 2023
* Linear Classifier: An Often-Forgotten Baseline for Text Classification
